{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T16:08:50.027200Z",
     "start_time": "2020-09-18T16:08:44.555848Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T16:16:16.697565Z",
     "start_time": "2020-09-18T16:16:14.577360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved EPA data summary csv at EPA_data_descriptions.csv\n",
      "      measurement                                         filename  \\\n",
      "0     Temperature    ../raw_data/Temperature_20170101_20171231.csv   \n",
      "1        Pressure       ../raw_data/Pressure_20200101_20201231.csv   \n",
      "2           PM2.5          ../raw_data/PM2.5_20170101_20171231.csv   \n",
      "3             NO2            ../raw_data/NO2_20200101_20201231.csv   \n",
      "4              CO             ../raw_data/CO_20180101_20181231.csv   \n",
      "5        Pressure       ../raw_data/Pressure_20180101_20181231.csv   \n",
      "6             NO2            ../raw_data/NO2_20180101_20181231.csv   \n",
      "7              CO             ../raw_data/CO_20200101_20201231.csv   \n",
      "8            PM10           ../raw_data/PM10_20160101_20161231.csv   \n",
      "9        Humidity       ../raw_data/Humidity_20170101_20171231.csv   \n",
      "10      Windspeed      ../raw_data/Windspeed_20160101_20161231.csv   \n",
      "11            SO2            ../raw_data/SO2_20190101_20191231.csv   \n",
      "12  Winddirection  ../raw_data/Winddirection_20190101_20191231.csv   \n",
      "13  Winddirection  ../raw_data/Winddirection_20180101_20181231.csv   \n",
      "14       Humidity       ../raw_data/Humidity_20160101_20161231.csv   \n",
      "15      Windspeed      ../raw_data/Windspeed_20170101_20171231.csv   \n",
      "16            SO2            ../raw_data/SO2_20180101_20181231.csv   \n",
      "17           PM10           ../raw_data/PM10_20170101_20171231.csv   \n",
      "\n",
      "                            units               duration  \n",
      "0              Degrees Centigrade                24 HOUR  \n",
      "1           Millimeters (mercury)                24 HOUR  \n",
      "2     Micrograms/cubic meter (LC)          24-HR BLK AVG  \n",
      "3               Parts per billion                 1 HOUR  \n",
      "4               Parts per million  8-HR RUN AVG END HOUR  \n",
      "5           Millimeters (mercury)                24 HOUR  \n",
      "6               Parts per billion                 1 HOUR  \n",
      "7               Parts per million  8-HR RUN AVG END HOUR  \n",
      "8   Micrograms/cubic meter (25 C)          24-HR BLK AVG  \n",
      "9       Percent relative humidity                 1 HOUR  \n",
      "10                          Knots                 1 HOUR  \n",
      "11              Parts per billion          24-HR BLK AVG  \n",
      "12                Degrees Compass                 1 HOUR  \n",
      "13                Degrees Compass                 1 HOUR  \n",
      "14      Percent relative humidity                 1 HOUR  \n",
      "15                          Knots                 1 HOUR  \n",
      "16              Parts per billion          24-HR BLK AVG  \n",
      "17  Micrograms/cubic meter (25 C)          24-HR BLK AVG  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_data_filename(filename):\n",
    "    filename = os.path.basename(filename)\n",
    "    no_extension = filename[:-4]\n",
    "    measurement, start_date, end_date = no_extension.split('_')\n",
    "    return measurement\n",
    "\n",
    "def load_files(max_files=2):\n",
    "    raw_data_dir = os.path.join('..', 'raw_data')\n",
    "    data_files = os.listdir(raw_data_dir)\n",
    "    \n",
    "    df_dict = {}\n",
    "    for i, path_name in enumerate(data_files):\n",
    "        # default to loading a single file\n",
    "        if i > max_files - 1:\n",
    "            break\n",
    "        full_path = os.path.join(raw_data_dir, path_name)\n",
    "        measurement = parse_data_filename(full_path)\n",
    "        key = '{}&{}'.format(measurement, full_path)\n",
    "        df = pd.read_csv(\n",
    "                full_path, \n",
    "                dtype={'date_local': str, \n",
    "                       'latitude': float,\n",
    "                       'longitude':float,\n",
    "                       'arithmetic_mean': float,\n",
    "                       'units_of_measure': str,\n",
    "                       'sample_duration': str \n",
    "                      })\n",
    "        \n",
    "        # don't intake any of the 0-length csv files. \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        df_dict[key] = df\n",
    "            \n",
    "    return df_dict\n",
    "    \n",
    "    \n",
    "def make_summary_table(df_dict, filename='EPA_data_descriptions.csv'):\n",
    "    list_of_files = []\n",
    "    for key in df_dict:\n",
    "        measurement, file_path = key.split('&')\n",
    "        df = df_dict[key]\n",
    "        append_dict = {'measurement': measurement, \n",
    "             'filename': file_path, \n",
    "             'units': df.head(1).iloc[0]['units_of_measure'], \n",
    "             'duration': df.head(1).iloc[0]['sample_duration']        \n",
    "            }\n",
    "        #print(append_dict)\n",
    "        list_of_files.append(append_dict)\n",
    "        \n",
    "    summary_df = pd.DataFrame(list_of_files)\n",
    "    summary_df.to_csv(filename)\n",
    "    print('Saved EPA data summary csv at {}'.format(filename))\n",
    "    \n",
    "    print(summary_df)\n",
    "        \n",
    "    \n",
    "df_dict = load_files(max_files=20)\n",
    "\n",
    "make_summary_table(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T16:30:44.887446Z",
     "start_time": "2020-09-18T16:30:42.916768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_local    365\n",
      "latitude       57\n",
      "longitude      57\n",
      "dtype: int64\n",
      "date_local    131\n",
      "latitude       52\n",
      "longitude      52\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       69\n",
      "longitude      69\n",
      "dtype: int64\n",
      "date_local    182\n",
      "latitude       94\n",
      "longitude      94\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       69\n",
      "longitude      69\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       55\n",
      "longitude      55\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude      103\n",
      "longitude     103\n",
      "dtype: int64\n",
      "date_local    183\n",
      "latitude       59\n",
      "longitude      59\n",
      "dtype: int64\n",
      "date_local    366\n",
      "latitude       79\n",
      "longitude      79\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       99\n",
      "longitude      99\n",
      "dtype: int64\n",
      "date_local    366\n",
      "latitude      158\n",
      "longitude     158\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       28\n",
      "longitude      28\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude      164\n",
      "longitude     164\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude      161\n",
      "longitude     161\n",
      "dtype: int64\n",
      "date_local    366\n",
      "latitude      105\n",
      "longitude     105\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude      160\n",
      "longitude     160\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       30\n",
      "longitude      30\n",
      "dtype: int64\n",
      "date_local    365\n",
      "latitude       84\n",
      "longitude      84\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def examine_lat_lon(summary_file='EPA_data_descriptions.csv'):\n",
    "    summary_df = pd.read_csv(summary_file)\n",
    "    total_date_lat_lon = pd.DataFrame([], columns=['date', 'lat', 'lon'])\n",
    "    date_lat_lon_list = []\n",
    "    for row in summary_df.iterrows():\n",
    "        filename = row[1]['filename']\n",
    "        df = pd.read_csv(filename)\n",
    "        print(df[['date_local', 'latitude', 'longitude']].nunique())\n",
    "        #print(df.head(1).iloc[0][['date_local', 'latitude', 'longitude']])\n",
    "        date_lat_lon_list.append(df[['date_local', 'latitude', 'longitude']])\n",
    "        \n",
    "\n",
    "    total_date_lat_lon = pd.concat(date_lat_lon_list)\n",
    "    \n",
    "    return total_date_lat_lon\n",
    "\n",
    "total_date_lat_lon = examine_lat_lon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T16:27:50.092348Z",
     "start_time": "2020-09-18T16:27:49.936718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_local    1644\n",
       "latitude       255\n",
       "longitude      255\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_date_lat_lon.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T06:43:02.646267Z",
     "start_time": "2020-09-18T06:43:02.638109Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def describe_nulls(df, show_nulls_table=False):    \n",
    "    print(\"Total rows: \\n\\t{:,}\".format(len(df)))\n",
    "    \n",
    "    rows_with_any_NA = len(df[df.isnull().any(axis=1)])\n",
    "    print(\"Rows; percent of rows that have *at least 1* missing value: \\n\\t{:,}; {}%\".format(\n",
    "        rows_with_any_NA, 100 * round(rows_with_any_NA / len(df), 2)))\n",
    "    \n",
    "    \n",
    "    if show_nulls_table:\n",
    "        nunique = df.nunique() # just compute this once.\n",
    "    \n",
    "        display(pd.DataFrame({\n",
    "            'dtype': df.dtypes,\n",
    "            'Number of nulls': df.isnull().sum(),\n",
    "            '% null': 100 * round(df.isnull().sum() / len(df), 4),\n",
    "            'Distinct Values': nunique,\n",
    "            '% Distinct Values': 100 * round(nunique / len(df), 4),\n",
    "        }))\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T15:29:18.769673Z",
     "start_time": "2020-09-18T15:29:18.748442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 3}\n"
     ]
    }
   ],
   "source": [
    "a = {}\n",
    "a[(1,2)] = 3\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
