{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T05:46:11.138976Z",
     "start_time": "2020-09-18T05:46:09.911810Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T15:41:03.838132Z",
     "start_time": "2020-09-18T15:40:58.427904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved EPA data summary csv at EPA_data_descriptions.csv\n",
      "   measurement                             filename  \\\n",
      "0        PM2.5    Temperature_20170101_20171231.csv   \n",
      "1        PM2.5       Pressure_20200101_20201231.csv   \n",
      "2        PM2.5          PM2.5_20170101_20171231.csv   \n",
      "3        PM2.5            NO2_20200101_20201231.csv   \n",
      "4        PM2.5             CO_20180101_20181231.csv   \n",
      "5        PM2.5       Pressure_20180101_20181231.csv   \n",
      "6        PM2.5            NO2_20180101_20181231.csv   \n",
      "7        PM2.5             CO_20200101_20201231.csv   \n",
      "8        PM2.5           PM10_20160101_20161231.csv   \n",
      "9        PM2.5       Humidity_20170101_20171231.csv   \n",
      "10       PM2.5      Windspeed_20160101_20161231.csv   \n",
      "11       PM2.5            SO2_20190101_20191231.csv   \n",
      "12       PM2.5  Winddirection_20190101_20191231.csv   \n",
      "13       PM2.5             O3_20160101_20161231.csv   \n",
      "14       PM2.5             O3_20170101_20171231.csv   \n",
      "15       PM2.5  Winddirection_20180101_20181231.csv   \n",
      "16       PM2.5       Humidity_20160101_20161231.csv   \n",
      "17       PM2.5      Windspeed_20170101_20171231.csv   \n",
      "18       PM2.5            SO2_20180101_20181231.csv   \n",
      "19       PM2.5           PM10_20170101_20171231.csv   \n",
      "\n",
      "                          units       duration  \n",
      "0   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "1   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "2   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "3   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "4   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "5   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "6   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "7   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "8   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "9   Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "10  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "11  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "12  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "13  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "14  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "15  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "16  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "17  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "18  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n",
      "19  Micrograms/cubic meter (LC)  24-HR BLK AVG  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_data_filename(filename):\n",
    "    no_extension = filename[:-4]\n",
    "    measurement, start_date, end_date = no_extension.split('_')\n",
    "    return measurement\n",
    "\n",
    "def load_files(max_files=2):\n",
    "    raw_data_dir = os.path.join(os.path.dirname(os.getcwd()), 'raw_data')\n",
    "    data_files = os.listdir(raw_data_dir)\n",
    "    \n",
    "    df_dict = {}\n",
    "    for i, path_name in enumerate(data_files):\n",
    "        # default to loading a single file\n",
    "        if i > max_files - 1:\n",
    "            break\n",
    "        full_path = os.path.join(raw_data_dir, s)\n",
    "        measurement = parse_data_filename(s)\n",
    "        key = '{}&{}'.format(measurement, path_name)\n",
    "        df = pd.read_csv(\n",
    "                full_path, \n",
    "                dtype={'date_local': str, \n",
    "                       'latitude': float,\n",
    "                       'longitude':float,\n",
    "                       'arithmetic_mean': float,\n",
    "                       'units_of_measure': str,\n",
    "                       'sample_duration': str \n",
    "                      })\n",
    "        \n",
    "        # don't intake any of the 0-length csv files. \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        df_dict[key] = df\n",
    "            \n",
    "    return df_dict\n",
    "    \n",
    "    \n",
    "def make_summary_table(df_dict, filename='EPA_data_descriptions.csv'):\n",
    "    list_of_files = []\n",
    "    for key in df_dict:\n",
    "        measurement, file_path = key.split('&')\n",
    "        df = df_dict[key]\n",
    "        append_dict = {'measurement': measurement, \n",
    "             'filename': file_path, \n",
    "             'units': df.head(1).iloc[0]['units_of_measure'], \n",
    "             'duration': df.head(1).iloc[0]['sample_duration']        \n",
    "            }\n",
    "        #print(append_dict)\n",
    "        list_of_files.append(append_dict)\n",
    "        \n",
    "    summary_df = pd.DataFrame(list_of_files)\n",
    "    summary_df.to_csv(filename)\n",
    "    print('Saved EPA data summary csv at {}'.format(filename))\n",
    "    \n",
    "    print(summary_df)\n",
    "        \n",
    "    \n",
    "df_dict = load_files(max_files=20)\n",
    "\n",
    "# for key in df_dict:\n",
    "#     print(key)\n",
    "\n",
    "make_summary_table(df_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T06:43:02.646267Z",
     "start_time": "2020-09-18T06:43:02.638109Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def describe_nulls(df, show_nulls_table=False):    \n",
    "    print(\"Total rows: \\n\\t{:,}\".format(len(df)))\n",
    "    \n",
    "    rows_with_any_NA = len(df[df.isnull().any(axis=1)])\n",
    "    print(\"Rows; percent of rows that have *at least 1* missing value: \\n\\t{:,}; {}%\".format(\n",
    "        rows_with_any_NA, 100 * round(rows_with_any_NA / len(df), 2)))\n",
    "    \n",
    "    \n",
    "    if show_nulls_table:\n",
    "        nunique = df.nunique() # just compute this once.\n",
    "    \n",
    "        display(pd.DataFrame({\n",
    "            'dtype': df.dtypes,\n",
    "            'Number of nulls': df.isnull().sum(),\n",
    "            '% null': 100 * round(df.isnull().sum() / len(df), 4),\n",
    "            'Distinct Values': nunique,\n",
    "            '% Distinct Values': 100 * round(nunique / len(df), 4),\n",
    "        }))\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T15:29:18.769673Z",
     "start_time": "2020-09-18T15:29:18.748442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 3}\n"
     ]
    }
   ],
   "source": [
    "a = {}\n",
    "a[(1,2)] = 3\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
